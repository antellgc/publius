{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('chained_assignment',None)\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ham = pd.read_csv(\"../raw_data/hamilton.csv\")\n",
    "df_mad = pd.read_csv(\"../raw_data/madison.csv\")\n",
    "df_jay = pd.read_csv(\"../raw_data/jay.csv\")\n",
    "\n",
    "def split_sentence(df):\n",
    "    all_sentences = []\n",
    "    for t in df['Text']:\n",
    "        sentences = t.split('.')\n",
    "        for s in sentences:\n",
    "            if len(s.strip()) > 10:\n",
    "                all_sentences.append(s.strip())\n",
    "    return all_sentences\n",
    "\n",
    "def expand_df(df):\n",
    "    df_out = pd.DataFrame()\n",
    "    sentences = split_sentence(df)\n",
    "    df_out.loc[:,'sentence'] = sentences\n",
    "    df_out.loc[:,'author'] = df['Author'][0]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split\n",
    "Many of the Federalist Papers are multi-part installments or continuations of earlier subjects. This could cause problems when it comes to train/test splitting because we want our classifier to identify authorship and not similar thematic content. Accordingly, selected the indices for splitting the datasets that ensure that continuations of the same subject were not assigned to both training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Hamilton': 51, 'Madison': 15, 'Hamilton or Madison': 11, 'Jay': 5, 'Hamilton and Madison': 3})\n",
      "(51, 5) (15, 5) (5, 5)\n"
     ]
    }
   ],
   "source": [
    "print(Counter(df.Author))\n",
    "\n",
    "# select according to author\n",
    "df_ham = df[df['Author'] == 'Hamilton'].reset_index().drop(['index'], axis=1)\n",
    "df_mad = df[df['Author'] == 'Madison'].reset_index().drop(['index'], axis=1)\n",
    "df_jay = df[df['Author'] == 'Jay'].reset_index().drop(['index'], axis=1)\n",
    "\n",
    "print(df_ham.shape, df_mad.shape, df_jay.shape)\n",
    "\n",
    "# assign first 30 papers from Hamilton to training\n",
    "df_ham_train = df_ham.iloc[:30,:].reset_index().drop(['index'], axis=1)\n",
    "df_ham_test = df_ham.iloc[30:,:].reset_index().drop(['index'], axis=1)\n",
    "\n",
    "# assign first 10 papers from Madison to training\n",
    "df_mad_train = df_mad.iloc[:10,:].reset_index().drop(['index'], axis=1)\n",
    "df_mad_test = df_mad.iloc[10:,:].reset_index().drop(['index'], axis=1)\n",
    "\n",
    "# assign first 3 papers from Madison to training\n",
    "df_jay_train = df_jay.iloc[:4,:].reset_index().drop(['index'], axis=1)\n",
    "df_jay_test = df_jay.iloc[4:,:].reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand dataset into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# expand dataframe to sentence-level\n",
    "df_ham_train_passages = expand_df(df_ham_train)\n",
    "df_mad_train_passages = expand_df(df_mad_train)\n",
    "df_jay_train_passages = expand_df(df_jay_train)\n",
    "df_ham_test_passages = expand_df(df_ham_test)\n",
    "df_mad_test_passages = expand_df(df_mad_test)\n",
    "df_jay_test_passages = expand_df(df_jay_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Hamilton': 1597, 'Madison': 787, 'Jay': 145})\n",
      "Counter({'Hamilton': 1384, 'Madison': 326, 'Jay': 55})\n"
     ]
    }
   ],
   "source": [
    "# concatenate into training and testing sets\n",
    "df_train = pd.concat([df_ham_train_passages, df_mad_train_passages, df_jay_train_passages], axis=0)\n",
    "df_test = pd.concat([df_ham_test_passages, df_mad_test_passages, df_jay_test_passages], axis=0)\n",
    "\n",
    "print(Counter(df_train['author']))\n",
    "print(Counter(df_test['author']))\n",
    "\n",
    "df_train.to_csv('../data/train.csv', index=False)\n",
    "df_test.to_csv('../data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Hamilton': 750, 'Madison': 750})\n",
      "Counter({'Hamilton': 300, 'Madison': 300})\n"
     ]
    }
   ],
   "source": [
    "df_ham_train_750 = df_ham_train_passages.sample(n=750, random_state=1).reset_index().drop(['index'], axis=1)\n",
    "df_mad_train_750 = df_mad_train_passages.sample(n=750, random_state=1).reset_index().drop(['index'], axis=1)\n",
    "df_ham_test_300 = df_ham_test_passages.sample(n=300, random_state=1).reset_index().drop(['index'], axis=1)\n",
    "df_mad_test_300 = df_mad_test_passages.sample(n=300, random_state=1).reset_index().drop(['index'], axis=1)\n",
    "\n",
    "df_train_balanced = pd.concat([df_ham_train_750, df_mad_train_750], axis=0)\n",
    "df_test_balanced = pd.concat([df_ham_test_300, df_mad_test_300], axis=0)\n",
    "\n",
    "print(Counter(df_train_balanced['author']))\n",
    "print(Counter(df_test_balanced['author']))\n",
    "\n",
    "df_train_balanced.to_csv('../data/train_balanced.csv', index=False)\n",
    "df_test_balanced.to_csv('../data/test_balanced.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
