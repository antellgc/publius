{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "pd.set_option('chained_assignment',None)\n",
    "\n",
    "# global variables\n",
    "url = \"https://www.congress.gov/resources/display/content/The+Federalist+Papers\"\n",
    "author_names = ['Madison','Hamilton','Jay','Hamilton and Madison','Hamilton or Madison']\n",
    "raw_data_path = \"../raw_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup_from_url(url):\n",
    "    \"\"\"\n",
    "    Query a given url and return the page content as BeautifulSoup object.\n",
    "    Parameters:\n",
    "        url: the url to be scraped\n",
    "    Returns:\n",
    "        soup: a BeautifulSoup object\n",
    "    \"\"\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def parse_federalist_text(soup):\n",
    "    \"\"\"\n",
    "    Extract the text information based on known structure of Federalist papers.\n",
    "    Parameters:\n",
    "        soup: a BeautifulSoup object\n",
    "    Returns:\n",
    "        all_texts: a list of the scraped texts\n",
    "    \"\"\"\n",
    "    text_block = soup.find_all(\"div\", class_=\"wiki-content\")[0].get_text()\n",
    "    all_texts = []\n",
    "    n = 1\n",
    "    for string in text_block.split('|| Federalist No.')[1:]:\n",
    "        string = string.replace(u'\\xa0', u' ')\n",
    "        string = '|| Federalist No.' + string\n",
    "        string = string.split('To the People of the State of New York:')[1]\n",
    "        string = string.split('â†‘ Back to Top')[0]\n",
    "        string = string.split('PUBLIUS')[0]\n",
    "        all_texts.append(string)\n",
    "        n += 1\n",
    "    return all_texts\n",
    "\n",
    "def extract_federalist_table(soup):\n",
    "    \"\"\"\n",
    "    Extract table information and return as list object.\n",
    "    Parameters:\n",
    "        soup: a BeautifulSoup object\n",
    "    Returns:\n",
    "        all_rows: a list of lists containing table information\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    row = []\n",
    "    all_rows = []\n",
    "    for i in soup.find_all(\"td\", class_=\"confluenceTd\"):\n",
    "        count += 1\n",
    "        if count%5 != 0:\n",
    "            row.append(i.get_text())\n",
    "        else:\n",
    "            row.append(i.get_text())\n",
    "            all_rows.append(row)\n",
    "            row = []\n",
    "    return all_rows\n",
    "\n",
    "def federalist_data_to_dataframe(all_rows, all_texts):\n",
    "    \"\"\"\n",
    "    Store table data and text data into a single pandas dataframe.\n",
    "    Parameters:\n",
    "        all_rows: a list of lists containing table information\n",
    "        all_texts: a list of the scraped texts\n",
    "    Returns:\n",
    "        federalist_dataframe: a pandas dataframe combining the two data sources\n",
    "    \"\"\"\n",
    "    federalist_dataframe = pd.DataFrame(all_rows)\n",
    "    federalist_dataframe.columns = ['No.','Title','Author','Publication','Date']\n",
    "    federalist_dataframe.loc[:,'Text'] = all_texts\n",
    "    federalist_dataframe.loc[:,'Length'] = federalist_dataframe['Text'].apply(len)\n",
    "    # fix Madison entries\n",
    "    federalist_dataframe['Author'] = federalist_dataframe['Author'].apply(lambda x: 'Madison' if 'Madison' in x else x)\n",
    "    return federalist_dataframe\n",
    "\n",
    "def select_papers_by_author(federalist_dataframe, author_name):\n",
    "    \"\"\"Provide name and return dataframe containing only this author.\"\"\"\n",
    "    author_dataframe = federalist_dataframe[federalist_dataframe['Author'] == author_name].reset_index().drop(['index'], axis=1)\n",
    "    return author_dataframe\n",
    "\n",
    "def save_to_path(author_dataframe, filename, raw_data_path):\n",
    "    \"\"\"Provide dataframe and save to filename\"\"\"\n",
    "    save_path = raw_data_path + filename + \".csv\"\n",
    "    author_dataframe.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = get_soup_from_url(url)\n",
    "all_texts = parse_federalist_text(soup)\n",
    "all_rows = extract_federalist_table(soup)\n",
    "federalist_dataframe = federalist_data_to_dataframe(all_rows, all_texts)\n",
    "\n",
    "federalist_dataframe['Author'] = federalist_dataframe['Author'].apply(lambda x: 'Madison' if 'Madison' in x else x)\n",
    "\n",
    "\n",
    "df = federalist_dataframe[federalist_dataframe['Author'].str.contains(\"Madison\")]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Hamilton': 51, 'Jay': 5, 'Madison': 29})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(federalist_dataframe[\"Author\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(url, author_names, raw_data_path):\n",
    "    soup = get_soup_from_url(url)\n",
    "    all_texts = parse_federalist_text(soup)\n",
    "    all_rows = extract_federalist_table(soup)\n",
    "    federalist_dataframe = federalist_data_to_dataframe(all_rows, all_texts)\n",
    "    # iterates through the authors of interest\n",
    "    for author_name in author_names:\n",
    "        author_dataframe = select_papers_by_author(federalist_dataframe, author_name)\n",
    "        print(author_name, author_dataframe.shape)\n",
    "        save_to_path(author_dataframe, author_name, raw_data_path)\n",
    "    print(\"Compelete\")\n",
    "\n",
    "main(url, author_names, raw_data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
